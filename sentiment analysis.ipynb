{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析：输入一段文本(电影评论)到一个训练好的模型中，输出这段模型是pos还是neg。\n",
    "使用的数据集是IMDB，在torchtext中这个包中可以导入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理一下思路：对于这个任务，数据是化成三分，train，valid，test。然后对train中最常见的25000个单词建立一个vocab。vocab的作用就是\n",
    "将每一个单词映射成一个数字。\n",
    "在建立模型的时候，首先是有一个emdedding层，这个层输入一个单词对应的数字，便会得到一个词向量，对于一段评论，输入这段文本中的每个词都会\n",
    "得到一个词向量，对于所有单词的词向量进行平均就会得到整段文本的词向量，在将平均词向量输入到一个线性变换层中，变得到了最终输出的pos还是neg，也就是0还是1.这就是word_avg模型的思路，其余的RNN、CNN模型的思路也差不离。求平均的时候用的avg_pool2d，注意维度的变化。\n",
    "注意：在输入模型中的时候是一个batch一个batch输入的，且是长度差不多的句子组成一个batch，这些工作都是参数设置的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torchtext import data\n",
    "\n",
    "SEED=1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fb8836507b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=data.Field(tokenize='spacy',tokenizer_language='en_core_web_sm',)#Field决定了数据会如何被处理。\n",
    "    \n",
    "LABEL=data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets#导入数据集，torchtext这个包中含有很多数据，这里使用IMDB数据，划分成train，test 数据。\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples 25000\n",
      "number of test examples 25000\n"
     ]
    }
   ],
   "source": [
    "print('number of training examples {}'.format(len(train_data)))\n",
    "print('number of test examples {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Like', 'his', 'elder', 'brothers', ',', 'Claude', 'Sautet', 'and', 'Jean', '-', 'Pierre', 'Melville', ',', 'Alain', 'Corneau', 'began', 'to', 'cut', 'his', 'teeth', 'in', 'French', 'cinema', 'with', 'a', 'series', 'of', 'fine', 'thrillers', ':', '\"', 'la', 'Menace', '\"', '(', '1977', ')', 'and', '\"', 'Série', 'Noire', '\"', '(', '1979', ')', 'among', 'others', '.', '\"', 'Police', 'Python', '357', '\"', 'is', 'a', 'good', 'example', 'of', 'how', 'Corneau', 'conceived', 'and', 'shot', 'his', 'works', 'at', 'this', 'time', 'of', 'his', 'career', '.', 'They', 'had', 'a', 'splendid', 'cinematography', ',', 'painstaking', 'screenplays', 'and', 'a', 'sophisticated', 'directing', 'elaborated', 'for', 'efficiency', \"'s\", 'sake.<br', '/><br', '/>The', 'police', 'superintendent', 'Ferrot', '(', 'Yves', 'Montand', ')', 'is', 'a', 'cop', 'with', 'unconventional', 'methods', 'who', 'usually', 'works', 'all', 'alone', '.', 'He', 'makes', 'the', 'acquaintance', 'of', 'a', 'young', 'woman', 'Sylvia', 'Léopardi', '(', 'Stefania', 'Sandrelli', ')', 'and', 'becomes', 'her', 'lover', 'while', 'ignoring', 'that', 'she', 'has', 'another', 'lover', ':', 'his', 'superior', 'Ganay', '(', 'François', 'Périer', ')', '.', 'When', 'the', 'latter', 'learns', 'it', ',', 'he', 'kills', 'her', 'in', 'a', 'fit', 'of', 'anger', '.', 'Ferrot', 'has', 'to', 'investigate', 'the', 'murder', 'and', 'all', 'the', 'clues', 'are', 'inexorably', 'against', 'him', '...', '<br', '/><br', '/>One', 'could', 'deem', 'that', 'this', 'kind', 'of', 'far', '-', 'fetched', 'story', 'is', \"n't\", 'exempt', 'from', 'glitches', 'and', 'sometimes', ',', 'one', 'can', 'see', 'right', 'through', 'it', 'but', 'Corneau', \"'s\", 'pedantic', 'directorial', 'style', 'helps', 'to', 'conjure', 'up', 'a', 'stifling', ',', 'dusky', 'atmosphere', '.', 'The', 'first', 'part', 'of', 'the', 'film', 'before', 'the', 'night', 'of', 'the', 'murder', 'might', 'seem', 'uninteresting', 'and', 'however', ',', 'it', 'is', 'crucial', 'for', 'what', 'will', 'follow', 'this', 'key', '-', 'moment', '.', 'Corneau', 'falls', 'back', 'on', 'a', 'sober', 'treatment', 'with', 'rather', 'sparse', 'moments', 'and', 'short', 'appearances', 'by', 'secondary', ',', 'minor', 'characters', 'whom', 'the', 'viewer', 'will', 'see', 'again', 'during', 'the', 'investigation', '.', 'In', 'spite', 'of', 'drawbacks', ',', 'Corneau', 'and', 'his', 'scenarist', 'Daniel', 'Boulanger', 'penned', 'a', 'deft', 'story', '.', 'Ménard', '(', 'Mathieu', 'Carrière', ')', 'who', 'sometimes', 'expresses', 'his', 'surprise', 'because', 'Ferrot', 'keeps', 'a', 'relatively', 'low', 'profile', 'during', 'the', 'investigation', '.', 'But', 'his', 'superior', 'knows', 'that', 'he', 'usually', 'works', 'alone', '.', 'Actually', ',', 'Ferrot', 'has', 'to', 'find', 'solid', 'tricks', 'to', 'muddy', 'the', 'waters', 'and', 'so', 'to', 'exonerate', 'himself', '.', 'Eventually', ',', 'the', 'chief', 'idea', 'of', 'the', 'film', 'concerns', 'Ferrot', 'himself', '.', 'He', \"'s\", 'a', 'cop', 'who', 'bit', 'by', 'bit', 'loses', 'his', 'identity', 'and', 'finds', 'himself', 'in', 'the', 'heart', 'of', 'a', 'terrible', 'depersonalization', '.', 'It', 'is', 'epitomized', 'by', 'the', 'moment', 'when', 'he', 'throws', 'himself', 'acid', 'on', 'his', 'face', 'so', 'that', 'witnesses', 'wo', \"n't\", 'recognize', 'him', 'when', 'he', 'is', 'brought', 'face', 'to', 'face', 'with', 'them.<br', '/><br', '/>The', 'backdrop', 'of', 'this', 'thriller', ',', 'Orléans', 'is', 'efficiently', 'enhanced', 'by', 'Corneau', \"'s\", 'camera', 'and', 'helps', 'to', 'inspire', 'this', 'eerie', 'thriller', 'its', 'pernicious', 'charm', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))#已经分词分好了，vars表示返回属性和属性值的字典对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data,valid_data=train_data.split(random_state=random.seed(SEED))#默认的是7,3分。\n",
    "print(len(train_data))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500\n",
      "7500\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(valid_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步我们需要创建 vocabulary 。vocabulary 就是把每个单词一一映射到一个数字。\n",
    "我们使用最常见的25k个单词来构建我们的单词表，用max_size这个参数可以做到这一点。\n",
    "所有其他的单词都用<unk>来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [01:51, 7.71MB/s]                               \n",
      " 99%|█████████▉| 397966/400000 [00:30<00:00, 22595.76it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)#选取 训练数据中最常见的25000个词组成词向量表。\n",
    "#其中，vectors表示的是采用\"glove.6B.100d\"这个预训练好的向量进行参数初始化。从而加速训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "print(vars(TEXT.vocab)['vectors'].shape)#vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 203479), (',', 192700), ('.', 165658), ('a', 109189), ('and', 109099), ('of', 100605), ('to', 93433), ('is', 76108), ('in', 61269), ('I', 54280), ('it', 53950), ('that', 49195), ('\"', 43989), (\"'s\", 43324), ('this', 42548), ('-', 36975), ('/><br', 35514), ('was', 35036), ('as', 30305), ('movie', 29951)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])#int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)#string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)#生成数据的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.iterator.BucketIterator'>\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1247x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "print(type(train_iterator))\n",
    "print(next(iter(train_iterator)))#查看一个batch的数据的格式。seq_len*batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1035x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1035, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1246x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1246, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1064x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1064, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1557x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1557, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 939x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([939, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 783x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([783, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 835x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([835, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1131x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1131, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1155x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1155, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1015x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1015, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1111x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1111, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 874x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([874, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 871x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([871, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 927x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([927, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 739x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([739, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1017x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1017, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 650x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([650, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 918x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([918, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1128x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1128, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 867x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([867, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 805x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([805, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 847x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([847, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1202x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1202, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 754x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([754, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 785x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([785, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1182x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1182, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1193x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1193, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1032x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1032, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1136x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1136, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 745x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([745, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1118x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1118, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1223x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1223, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1139x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1139, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1144x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1144, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 869x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([869, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 789x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([789, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1167x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1167, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1041x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1041, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1089x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1089, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1194x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1194, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1019x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1019, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 880x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([880, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 860x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([860, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 609x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([609, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 656x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([656, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 958x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([958, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 876x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([876, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1019x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1019, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1120x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1120, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1166x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1166, 64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 883x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([883, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1069x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1069, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 763x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([763, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1180x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1180, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 907x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([907, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 707x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([707, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1607x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1607, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1126x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1126, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 872x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([872, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1307x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1307, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 872x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([872, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 565x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([565, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1176x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1176, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 873x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([873, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1070x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1070, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1698x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1698, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1161x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1161, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1080x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1080, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1041x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1041, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1182x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1182, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1206x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1206, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1123x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1123, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 874x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([874, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 980x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([980, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1158x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1158, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 657x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([657, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1187x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1187, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1162x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1162, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1951x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1951, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1004x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1004, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 940x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([940, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 714x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([714, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1827x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1827, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 984x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([984, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 921x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([921, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1103x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1103, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1188x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1188, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 933x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([933, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 886x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([886, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 979x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([979, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1142x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1142, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1098x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1098, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1140x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1140, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 832x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([832, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1145x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1145, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 753x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([753, 64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1016x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1016, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 860x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([860, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1012x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1012, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1109x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1109, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 825x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([825, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1196x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1196, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 812x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([812, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1065x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1065, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1476x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1476, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 935x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([935, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1126x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1126, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 838x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([838, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 755x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([755, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1454x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1454, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 997x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([997, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 915x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([915, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1155x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1155, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1150x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1150, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1164x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1164, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1152x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1152, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 689x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([689, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1060x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1060, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 815x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([815, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1172x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1172, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1091x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1091, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1038x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1038, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 773x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([773, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 878x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([878, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 711x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([711, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 920x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([920, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1081x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1081, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1133x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1133, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 988x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([988, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 994x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([994, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1022x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1022, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 861x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([861, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1365x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1365, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1080x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1080, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 605x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([605, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1045x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1045, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 965x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([965, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 814x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([814, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1172x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1172, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1084x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1084, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 796x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([796, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1064x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1064, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1172x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1172, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 892x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([892, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 688x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([688, 64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 954x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([954, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 970x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([970, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1285x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1285, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1186x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1186, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1249x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1249, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1103x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1103, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1025x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1025, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 976x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([976, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1179x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1179, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1051x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1051, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 945x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([945, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1332x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1332, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1158x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1158, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1104x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1104, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1148x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1148, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 927x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([927, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 972x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([972, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1096x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1096, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 855x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([855, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1115x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1115, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1263x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1263, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1041x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1041, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1135x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1135, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 944x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([944, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1058x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1058, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 696x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([696, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 957x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([957, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 889x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([889, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1122x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1122, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1019x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1019, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1172x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1172, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1197x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1197, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1077x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1077, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 950x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([950, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1015x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1015, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 775x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([775, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1016x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1016, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 914x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([914, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 946x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([946, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1016x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1016, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 753x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([753, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 615x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([615, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1007x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1007, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1154x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1154, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 929x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([929, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1067x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1067, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1068x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1068, 64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1014x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1014, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1083x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1083, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1005x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1005, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 987x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([987, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1098x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1098, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1175x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1175, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 872x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([872, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 910x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([910, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1048x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1048, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1035x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1035, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1223x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1223, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 889x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([889, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 980x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([980, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1125x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1125, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1184x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1184, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1200x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1200, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 744x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([744, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 990x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([990, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1171x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1171, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 2789x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([2789, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 2142x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([2142, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 986x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([986, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1248x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1248, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 672x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([672, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 866x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([866, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1120x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1120, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 747x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([747, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 967x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([967, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1254x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1254, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1743x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1743, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1106x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1106, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 811x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([811, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1044x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1044, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 950x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([950, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 958x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([958, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1146x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1146, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1201x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1201, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1156x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1156, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1027x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1027, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 873x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([873, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1122x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1122, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1126x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1126, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1205x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1205, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1088x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1088, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1018x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1018, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1074x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1074, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1097x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1097, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 944x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([944, 64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 694x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([694, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 876x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([876, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 735x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([735, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 817x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([817, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1012x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1012, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1117x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1117, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1207x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1207, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 717x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([717, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1211x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1211, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1074x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1074, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1150x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1150, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1143x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1143, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1108x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1108, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 874x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([874, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1167x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1167, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1079x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1079, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1061x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1061, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 989x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([989, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 738x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([738, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 28]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 627x28 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 28 (GPU 0)]\n",
      "torch.Size([627, 28])\n",
      "torch.Size([28])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1387x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1387, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 781x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([781, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1177x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1177, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1070x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1070, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1147x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1147, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1212x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1212, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1369x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1369, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 822x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([822, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1065x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1065, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1082x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1082, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 803x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([803, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 1013x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([1013, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 817x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([817, 64])\n",
      "torch.Size([64])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 820x64 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([820, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for example in train_iterator:#也就是每一列表示一个样本\n",
    "    print(example)\n",
    "    print(example.text.shape)\n",
    "    print(example.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面就是搭建word_avg模型了\n",
    "#一段评论,根据vocab转换成数字，一个单词对应一个数字，一个数字根据embedding找一个向量embed_size长度。\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class Wordavgmodel(nn.Module):\n",
    "    #下面这一段就是明白这个意思，但是写混乱了@@～～@@，推到，重来！！\n",
    "    def __init__(self,vocab_size,embed_size,pad_idx,outputdim):\n",
    "        super(Wordavgmodel,self).__init__()\n",
    "        \n",
    "#         #查官方文档的api，变可以知道输出是什么维度的\n",
    "#         embed=nn.Embedding(vocab_size,embed_size,padding_idx=pad_idx)#[seq_len,batch,embed_size]\n",
    "#         #为了输入到avg_pool2d中维度合适，需要进行维度转换\n",
    "#         embed=torch.perm(1,0,2)#[batch,seq_len,embed_size]\n",
    "#         avg=F.avg_pool2d(embed,(embed.shape[1],1)).squeeze(1)#[batch,1,embed_size]---->[batch,embed_size]\n",
    "#         self.linear=nn.Linear(avg,outputdim)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size,padding_idx=pad_idx)\n",
    "        self.fc=nn.Linear(embed_size,outputdim)\n",
    "    def forward(self,text):#这里text 的形式就是在一个batch里数据的格式，因为在model train的时候，是将一个batch的数据喂入\n",
    "        embed=self.embedding(text)#[seq_len,batch,embed_size] ，text的格式为[seq_len,batch_size]\n",
    "        #print(embed.shape)\n",
    "        embed=embed.permute(1,0,2)#[batch,seq_len,embed_size]\n",
    "        #avg_pool对所有单词的词向量做平均，这里F.avg_pool2d的池化维度就是取向量每一个位置的值\n",
    "        avg=F.avg_pool2d(embed,(embed.shape[1],1)).squeeze(1)#去除第二个的维度1.[batch size, embedding_dim]\n",
    "        return self.fc(avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM=len(TEXT.vocab)\n",
    "print(INPUT_DIM)\n",
    "print(TEXT.vocab.stoi[TEXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=len(TEXT.vocab)#25002\n",
    "EMBEDDING_DIM=100\n",
    "OUTPUTDIM=1\n",
    "PAD_IDX=TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model= Wordavgmodel(INPUT_DIM,EMBEDDING_DIM,PAD_IDX,OUTPUTDIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordavgmodel(\n",
      "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model) #embedding维度是这样的，但是放进去的text的维度是1247*64啊，计算出来的维度是[1247,64,100],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(2, 5)\n",
      "tensor([], size=(0, 5), grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "#nn.Embedding(l,h):生成一个l×h的矩阵，l表示单词个数，h为词嵌入的维度，是自定义的，\n",
    "#这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。\n",
    "from torch.autograd import Variable\n",
    "word_to_index={'hello':0,'world':1}\n",
    "hello_index=Variable(torch.LongTensor(word_to_index['hello']))\n",
    "embeds=nn.Embedding(2,5)\n",
    "print(embeds)\n",
    "print(embeds(hello_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2054, -1.6745, -0.6783,  ...,  0.6963,  1.8395, -1.0152],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.6114, -0.9283,  0.9795,  ..., -1.3320,  0.1223, -1.2474],\n",
      "        ...,\n",
      "        [-0.4985,  0.5306, -0.7786,  ...,  1.0312, -2.7757, -0.4136],\n",
      "        [ 0.5917,  0.3962, -0.2695,  ..., -0.2319, -0.0390, -0.2140],\n",
      "        [-0.8517, -0.8586, -1.2057,  ..., -0.7280,  1.1957,  1.2343]],\n",
      "       requires_grad=True)\n",
      "2500200\n",
      "Parameter containing:\n",
      "tensor([[-0.0092,  0.0752,  0.0202,  0.0121,  0.0342, -0.0687,  0.0566, -0.0823,\n",
      "         -0.0179,  0.0657,  0.0708,  0.0449,  0.0075,  0.0200,  0.0464, -0.0942,\n",
      "          0.0539, -0.0560, -0.0038,  0.0918,  0.0722, -0.0931, -0.0202, -0.0681,\n",
      "         -0.0495,  0.0195,  0.0484, -0.0016, -0.0002,  0.0677,  0.0120, -0.0918,\n",
      "         -0.0551, -0.0326, -0.0799,  0.0378, -0.0531,  0.0564,  0.0657,  0.0543,\n",
      "          0.0436,  0.0842,  0.0900, -0.0788, -0.0638, -0.0654, -0.0550,  0.0881,\n",
      "         -0.0095, -0.0542,  0.0195,  0.0240,  0.0543, -0.0960,  0.0629, -0.0875,\n",
      "          0.0102, -0.0382,  0.0331, -0.0513,  0.0698, -0.0755,  0.0272,  0.0178,\n",
      "          0.0444, -0.0287,  0.0165,  0.0533, -0.0928, -0.0350, -0.0347, -0.0843,\n",
      "          0.0470, -0.0779,  0.0748, -0.0674,  0.0827,  0.0606, -0.0233,  0.0273,\n",
      "         -0.0393,  0.0751, -0.0046, -0.0879, -0.0902, -0.0150, -0.0952, -0.0751,\n",
      "          0.0802, -0.0173, -0.0537, -0.0374, -0.0754, -0.0525, -0.0701, -0.0311,\n",
      "          0.0264,  0.0608, -0.0337, -0.0918]], requires_grad=True)\n",
      "100\n",
      "Parameter containing:\n",
      "tensor([0.0143], requires_grad=True)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)\n",
    "    print(p.numel())#numel()方法返回数组中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model has 2,500,301 model parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_model_parameters(model):\n",
    "    return sum( p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'the model has {count_model_parameters(model):,} model parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],\n",
       "        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],\n",
       "        [ 0.2525,  0.4068, -0.1437,  ..., -0.5324,  0.4820,  0.1396]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors #shape(25002,100)，运用了预训练的vectors=\"glove.6B.100d\"进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25002, 100])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],\n",
       "        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],\n",
       "        [ 0.2525,  0.4068, -0.1437,  ..., -0.5324,  0.4820,  0.1396]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型参数初始化\n",
    "pretrained_embedding=TEXT.vocab.vectors #vectors=\"glove.6B.100d\"\n",
    "model.embedding.weight.data.copy_(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(TEXT.vocab)#显示vocab的所有的属性和属性值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a model\n",
    "optimizer=torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()#计算二分类的输出和target之间的交叉熵，输出经过了sigmoid了。\n",
    "\n",
    "model=model.to(device)\n",
    "criterion=criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(y_preds,y):\n",
    "    \n",
    "    round_preds = torch.round(torch.sigmoid(y_preds))\n",
    "    correct = (round_preds==y).float()\n",
    "    acc = correct.sum()/len(correct)  \n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch.text).squeeze(1)\n",
    "        loss=criterion(y_pred,batch.label)\n",
    "        acc=binary_accuracy(y_pred,batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            y_pred=model(batch.text).squeeze(1)\n",
    "            loss=criterion(y_pred,batch.label)\n",
    "            acc=binary_accuracy(y_pred,batch.label)\n",
    "            \n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time,end_time):\n",
    "    elapsed_time=int(end_time-start_time)\n",
    "    elapsed_mins=int(elapsed_time/60)\n",
    "    elapsed_secs=int(elapsed_time-(elapsed_mins*60))#不够一分钟的那些秒\n",
    "    return elapsed_mins,elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01 | Epoch time:0m 5s\n",
      "\tTrain Loss:0.685| Train acc:59.97%\n",
      "\tVal Loss:0.6213701508812985:.3f | Valid acc:71.73%\n",
      "Epoch:02 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.641| Train acc:74.36%\n",
      "\tVal Loss:0.4979406969021943:.3f | Valid acc:76.66%\n",
      "Epoch:03 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.565| Train acc:79.21%\n",
      "\tVal Loss:0.47108364079968407:.3f | Valid acc:79.20%\n",
      "Epoch:04 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.493| Train acc:83.12%\n",
      "\tVal Loss:0.430426794214774:.3f | Valid acc:82.64%\n",
      "Epoch:05 | Epoch time:0m 5s\n",
      "\tTrain Loss:0.431| Train acc:85.91%\n",
      "\tVal Loss:0.41735472732176215:.3f | Valid acc:84.67%\n",
      "Epoch:06 | Epoch time:0m 5s\n",
      "\tTrain Loss:0.383| Train acc:87.87%\n",
      "\tVal Loss:0.4244473109305915:.3f | Valid acc:86.09%\n",
      "Epoch:07 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.346| Train acc:88.98%\n",
      "\tVal Loss:0.4416395543237864:.3f | Valid acc:86.57%\n",
      "Epoch:08 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.318| Train acc:89.74%\n",
      "\tVal Loss:0.4547139684022483:.3f | Valid acc:87.33%\n",
      "Epoch:09 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.292| Train acc:90.55%\n",
      "\tVal Loss:0.47526158543966585:.3f | Valid acc:87.49%\n",
      "Epoch:10 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.272| Train acc:91.30%\n",
      "\tVal Loss:0.4912670300168506:.3f | Valid acc:87.95%\n",
      "Epoch:11 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.252| Train acc:91.99%\n",
      "\tVal Loss:0.5085847577932527:.3f | Valid acc:88.19%\n",
      "Epoch:12 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.237| Train acc:92.45%\n",
      "\tVal Loss:0.525778437948833:.3f | Valid acc:88.35%\n",
      "Epoch:13 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.223| Train acc:92.96%\n",
      "\tVal Loss:0.5427192425576307:.3f | Valid acc:88.28%\n",
      "Epoch:14 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.211| Train acc:93.29%\n",
      "\tVal Loss:0.5605310391697843:.3f | Valid acc:88.73%\n",
      "Epoch:15 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.200| Train acc:93.83%\n",
      "\tVal Loss:0.5739146954679893:.3f | Valid acc:88.60%\n",
      "Epoch:16 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.186| Train acc:94.13%\n",
      "\tVal Loss:0.5905696508490433:.3f | Valid acc:88.83%\n",
      "Epoch:17 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.176| Train acc:94.49%\n",
      "\tVal Loss:0.603550075726994:.3f | Valid acc:88.73%\n",
      "Epoch:18 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.168| Train acc:94.93%\n",
      "\tVal Loss:0.6198414559712855:.3f | Valid acc:88.67%\n",
      "Epoch:19 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.157| Train acc:95.28%\n",
      "\tVal Loss:0.6352653761031264:.3f | Valid acc:88.84%\n",
      "Epoch:20 | Epoch time:0m 4s\n",
      "\tTrain Loss:0.149| Train acc:95.73%\n",
      "\tVal Loss:0.6544224882529954:.3f | Valid acc:88.83%\n"
     ]
    }
   ],
   "source": [
    "#训练wordavg\n",
    "NUM_EPOCHES=20\n",
    "best_valid_loss=float('inf')#正无穷大的数\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    start_time=time.time()\n",
    "    train_loss,train_acc=train(model,train_iterator,optimizer,criterion)\n",
    "    val_loss,val_acc=evaluate(model,valid_iterator,criterion)\n",
    "    end_time=time.time()\n",
    "    \n",
    "    epoch_min,epoch_secs=epoch_time(start_time,end_time)\n",
    "    \n",
    "    if val_loss<best_valid_loss:\n",
    "        best_valid_loss=val_loss\n",
    "        torch.save(model.state_dict(),'/home/control/Desktop/text8/wordavg-model.pth')\n",
    "    print(f'Epoch:{epoch+1:02} | Epoch time:{epoch_min}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss:{train_loss:.3f}| Train acc:{train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss:{val_loss}:.3f | Valid acc:{val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp=spacy.load('en')\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "def predicit_sentiment(sentence):\n",
    "    tokenized=[tok.text for tok in nlp.tokenizer(sentence)]#对一句话进行分词\n",
    "    indexed=[TEXT.vocab.stoi[i] for i in tokenized]#找到每一个词对应的索引（在词典中）\n",
    "    tensor=torch.LongTensor(indexed).to(device)\n",
    "    tensor=tensor.unsqueeze(1)#在第一个位置添加一个维度1\n",
    "    #这里必须加上一个维度1，是因为模型中forward(text)的text的格式是[seq_len,batch],所以为了送入模型是对的，维度必须符合\n",
    "    prediction=torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicit_sentiment('this film is terrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.216053537743827e-20"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicit_sentiment('this film is green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建一个RNN模型\n",
    "#rnn 经常作为一个编码器，对一个sentence进行编码，使用最后一个hidden state表示整个句子。\n",
    "#但是这个地方是双向rnn，所以有两个hidden，需要把两个hidden进行级联\n",
    "#把最后一个hidden通过一个线性变换f，预测句子的情感。\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout,pad_idx):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.embed=nn.Embedding(vocab_size,embedding_dim,padding_idx=pad_idx)\n",
    "        self.rnn=nn.LSTM(embedding_dim,hidden_dim,num_layers=n_layers,\n",
    "                         bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc=nn.Linear(hidden_dim*2,output_dim)#注意这里的维度设置和lstm是否是双向的有关系。\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,text):#这里的text就是iterator中每次取出来一个batch的数据。\n",
    "        embed=self.drop(self.embed(text))  ##[sent len, batch size, emb dim]     \n",
    "        output,(hidden, cell)=self.rnn(embed)#这里直接填入embed就好了。#因为是lstm所以输出是不一样的。\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        #当采用双向LSTM的输出，需要将最后的两个hidden进行级联，最后的forward和最后的backward\n",
    "        hidden=self.drop(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))#cat((a,b),0/1)，拼接两个张量，按照行或者列\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=len(TEXT.vocab)\n",
    "EMBEDDING_DIM=100\n",
    "HIDDEN_DIM=256\n",
    "OUTPUT_DIM=1\n",
    "N_LAYERS=2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "rnn_model = RNNModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embed): Embedding(25002, 100, padding_idx=1)\n",
      "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model parameters 4,810,857 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f'the model parameters {count_model_parameters(rnn_model):,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],\n",
      "        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],\n",
      "        [ 0.2525,  0.4068, -0.1437,  ..., -0.5324,  0.4820,  0.1396]])\n"
     ]
    }
   ],
   "source": [
    "rnn_model.embed.weight.data.copy_(pretrained_embedding)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "rnn_model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "rnn_model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(rnn_model.embed.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(rnn_model.parameters())\n",
    "rnn_model=rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01 | Epoch time:1m 17s\n",
      "\tTrain Loss:0.670| Train acc:58.60%\n",
      "\tVal. Loss:0.6905420968088053:.3f | Valid acc:58.80%\n",
      "Epoch:02 | Epoch time:1m 19s\n",
      "\tTrain Loss:0.628| Train acc:65.46%\n",
      "\tVal. Loss:0.6737146877636344:.3f | Valid acc:53.75%\n",
      "Epoch:03 | Epoch time:1m 19s\n",
      "\tTrain Loss:0.538| Train acc:73.62%\n",
      "\tVal. Loss:0.994186706967273:.3f | Valid acc:59.18%\n",
      "Epoch:04 | Epoch time:1m 20s\n",
      "\tTrain Loss:0.718| Train acc:52.25%\n",
      "\tVal. Loss:0.676621558807664:.3f | Valid acc:58.74%\n",
      "Epoch:05 | Epoch time:1m 17s\n",
      "\tTrain Loss:0.627| Train acc:63.08%\n",
      "\tVal. Loss:0.6819068208589392:.3f | Valid acc:52.39%\n",
      "Epoch:06 | Epoch time:1m 15s\n",
      "\tTrain Loss:0.517| Train acc:74.93%\n",
      "\tVal. Loss:0.45044662209890657:.3f | Valid acc:79.67%\n",
      "Epoch:07 | Epoch time:1m 22s\n",
      "\tTrain Loss:0.305| Train acc:87.75%\n",
      "\tVal. Loss:0.39604502398583846:.3f | Valid acc:84.23%\n",
      "Epoch:08 | Epoch time:1m 19s\n",
      "\tTrain Loss:0.254| Train acc:90.11%\n",
      "\tVal. Loss:0.4529498441744659:.3f | Valid acc:76.18%\n",
      "Epoch:09 | Epoch time:1m 19s\n",
      "\tTrain Loss:0.220| Train acc:91.44%\n",
      "\tVal. Loss:0.4759112883169772:.3f | Valid acc:83.07%\n",
      "Epoch:10 | Epoch time:1m 18s\n",
      "\tTrain Loss:0.181| Train acc:93.13%\n",
      "\tVal. Loss:0.3438403975155394:.3f | Valid acc:86.56%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHES=10\n",
    "best_valid_loss=float('inf')\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    start_time=time.time()\n",
    "    train_loss,train_acc=train(rnn_model,train_iterator,optimizer,criterion)\n",
    "    val_loss,val_acc=evaluate(rnn_model,valid_iterator,criterion)\n",
    "    end_time=time.time()\n",
    "    \n",
    "    epoch_min,epoch_secs=epoch_time(start_time,end_time)\n",
    "    \n",
    "    if val_loss<best_valid_loss:\n",
    "        best_valid_loss=val_loss\n",
    "        torch.save(rnn_model.state_dict(),'/home/control/Desktop/text8/lstm-model.pth')\n",
    "    print(f'Epoch:{epoch+1:02} | Epoch time:{epoch_min}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss:{train_loss:.3f}| Train acc:{train_acc*100:.2f}%')\n",
    "    print(f'\\tVal. Loss:{val_loss}:.3f | Valid acc:{val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:0.431 | test acc :84.356%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/home/control/Desktop/text8/wordavg-model.pth'))\n",
    "test_loss,test_acc=evaluate(model,test_iterator,criterion)\n",
    "print(f'test loss:{test_loss:.3f} | test acc :{test_acc*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss :0.379 | test acc:84.400%\n"
     ]
    }
   ],
   "source": [
    "rnn_model.load_state_dict(torch.load('/home/control/Desktop/text8/lstm-model.pth'))\n",
    "test_loss,test_acc = evaluate(rnn_model,test_iterator,criterion)\n",
    "print(f'test loss :{test_loss:.3f} | test acc:{test_acc*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modulelist中的module可以被主module所识别，但是普通list中的module不能被识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim,n_filters,filter_sizes,output_dim,dropout,pad_idx):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embed=nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.list=nn.ModuleList([nn.Conv2d(in_channels=1,out_channels=n_filters,\n",
    "                                          kernel_size=(fs,embedding_dim)) for fs in filter_sizes ])\n",
    "        self.fc=nn.Linear(len(filter_sizes)*n_filters,output_dim)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,text):#原本输入的text的格式是[seq len,batchsize]\n",
    "        text=text.permute(1,0)#[batch,seq_len]\n",
    "        embed=self.embed(text)#[batch,seq_len,embed_dim]\n",
    "        embed=embed.unsqueeze(1)#[batch,1,sen_len,embed_dim]\n",
    "        \n",
    "        convd = [F.relu(conv(embed)).squeeze(3) for conv in self.list]\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        poold=[F.max_pool1d(conv,conv.shape[2]).squeeze(2) for conv in convd]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.drop(torch.cat(poold,dim=1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "cnn_model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embed): Embedding(25002, 100, padding_idx=1)\n",
      "  (list): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],\n",
      "        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],\n",
      "        [ 0.2525,  0.4068, -0.1437,  ..., -0.5324,  0.4820,  0.1396]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_model.embed.weight.data.copy_(pretrained_embedding)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "cnn_model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "cnn_model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "cnn_model = cnn_model.to(device)\n",
    "print(cnn_model.embed.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.732 | Train Acc: 50.13%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 02 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.730 | Train Acc: 50.50%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.729 | Train Acc: 50.56%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.731 | Train Acc: 49.99%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.733 | Train Acc: 49.87%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 06 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.738 | Train Acc: 49.02%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.729 | Train Acc: 50.67%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.731 | Train Acc: 50.36%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 09 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.736 | Train Acc: 49.65%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n",
      "Epoch: 10 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.729 | Train Acc: 50.28%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.20%\n"
     ]
    }
   ],
   "source": [
    "optim=torch.optim.Adam(cnn_model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(cnn_model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(cnn_model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(cnn_model.state_dict(), 'CNN-model.pth')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.695 | Test Acc: 50.07%\n"
     ]
    }
   ],
   "source": [
    "cnn_model.load_state_dict(torch.load('CNN-model.pth'))\n",
    "test_loss, test_acc = evaluate(cnn_model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量：\n",
    "1. 离散式表示：one-hot编码，每一个单词为一个词向量，向量长度为整个词汇表。如,[0,0,0,0,1,0,0]\n",
    "2. 分布式表示。nn.Embedding(a,b)，a表示单词的个数，b表示词向量宽度，通过训练学习得到词向量，原理依据就是每一个词的出现的概率取决于\n",
    "   其上下文中词的概率的平均。\n",
    "自然语言模型：表示一个句子出现的概率，p(w1,w2,w3,,,wn)=p(w1)*p(w2|w1)*p(w3|w1,w2),,,p(wn|w1,w2,,,wn-1)，这种预测一个序列的概率输出，完全可以用RNN这种模型来做，因为RNN就是输入一个序列，通过计算（根据当前时刻输入和上一个时刻输出计算当前时刻输出），得到一个序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次要学习的东西:\n",
    "一个数据处理的包 torchtext\n",
    "如何搭建，RNN，LSTM，GRU等模型。\n",
    "包括RNN训练的技巧等、保存和读取模型等。\n",
    "###：模型输入是一串文字，输出也是一串文字，他们之间相差一个位置，语言模型的目标是根据之前的单词预测下一个单词，这就可以解释了\n",
    "我们通常把整个序列作为一个训练样本，所以总的误差就是每一步的误差的加和。\n",
    "RNN其实是很浅层的网络（目前见过的只有两三层的），但是却是很宽（通常一个序列会很长）的网络！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext #这是处理数据的一个包\n",
    "import torch\n",
    "import random\n",
    "from torchtext.vocab import Vectors\n",
    "import numpy as np\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(123)\n",
    "BATCH_SIZE=32\n",
    "EMBEDDING_SIZE=650#词向量的长度\n",
    "MAX_VOCAB_SIZE=50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.language_modeling.LanguageModelingDataset object at 0x7f08c44ec470>\n"
     ]
    }
   ],
   "source": [
    "TEXT=torchtext.data.Field(lower=True)#Field决定了如何处理数据，这相当于一个工具。lower表示，所有的单词会被小写。\n",
    "train,val,test=torchtext.datasets.LanguageModelingDataset.splits(path='/home/control/Desktop/text8',\n",
    "                    train='text8.train.txt',validation='text8.dev.txt',test='text8.test.txt',text_field=TEXT)\n",
    "#torchtext提供了LanguageModelingDataset这个类帮助处理数据集。\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.datasets.language_modeling.LanguageModelingDataset'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(type(train))\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train,max_size=MAX_VOCAB_SIZE)#build_vocab根据提供的训练集创建最高频的单词表，max_size限定单词数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size :50002\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size :{}'.format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPTTIterator可以连续地得到连贯的句子，BPTT的全程是back propagation throught time,就是在RNN中使用的反向传播算法，可以得到\n",
    "输出对U，V，W三个的求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50002\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE=len(TEXT.vocab)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的词汇表的长度是50000，按照之前的方式生成的词向量是50000维的，这样的是非常稀疏的，在这里的，使用的EMBEDDING_SIZE=650作为词向量的\n",
    "长度！，所以这里学习了一个embedding层，将50000映射成650，即学习了一个二维矩阵，(50000,650)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE=torch.device('cuda')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BPTTIterator object at 0x7f08c44ec320>\n"
     ]
    }
   ],
   "source": [
    "train_iter,val_iter,test_iter=torchtext.data.BPTTIterator.splits((train,val,test),batch_size=BATCH_SIZE,\n",
    "                            device=DEVICE,bptt_len=32,repeat=False,shuffle=True)\n",
    "print(train_iter)#是一个迭代器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可迭代对象（可用于for的）包含两类：一类是dict，list，tuple，str，set等。一类是generator,迭代器。迭代器不仅可用于for循环，还可以放入next中不断的取回下一个值。普通的dict、list等放入iter()中就是变成了迭代器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a rather fast decision one year or more is not unusual people who have only changed their name have a questionable legal status while most of the time this is perfectly sufficient\n",
      "rather fast decision one year or more is not unusual people who have only changed their name have a questionable legal status while most of the time this is perfectly sufficient there\n"
     ]
    }
   ],
   "source": [
    "it=iter(train_iter)\n",
    "batch=next(it)\n",
    "print(' '.join([ TEXT.vocab.itos[i] for i in batch.text[:,31].data]))\n",
    "#print(batch.text[:,1].data)\n",
    "print(' '.join([ TEXT.vocab.itos[i] for i in batch.target[:,31].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(32):\n",
    "#     print(' '.join([ TEXT.vocab.itos[i] for i in batch.text[:,i].data]))\n",
    "#     #print(batch.text[:,1].data)\n",
    "#     print(' '.join([ TEXT.vocab.itos[i] for i in batch.target[:,i].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yield 生成器。含有yield的就不是一个函数。而是一个生成器了。\n",
    "# def foo ():\n",
    "#     print('*'*20)\n",
    "#     while True:\n",
    "#         res= yield 4\n",
    "#         print(\"res:\",res)\n",
    "# g=foo()\n",
    "# print(next(g))\n",
    "# print('*'*20)\n",
    "# print(next(g))\n",
    "# print('*'*25)\n",
    "# print(next(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建一个RNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNNModel(nn.Module):\n",
    "    '''\n",
    "    一个嵌入层，encoding\n",
    "    一个循环神经网络层（RNN,LSTM,GRU）\n",
    "    一个解码层，将隐层映射为输出\n",
    "    一个dropout层，用于regularation。  \n",
    "    '''\n",
    "    def __init__(self,rnn_type,ntoken,ninp,nhid,nlayers,dropout=0.5):\n",
    "        \n",
    "        super(RNNModel,self).__init__()\n",
    "        self.encode=nn.Embedding(ntoken,ninp)#生成词向量，ntoken表示输入样本个数，ninp表示向量长度\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        #在搭建rnn层的时候，不是直接nn.RNN()，而是先看看rnn_type是否合适的！！\n",
    "        if rnn_type in ['LSTM','GRU']:\n",
    "            self.rnn=getattr(nn,rnn_type)(ninp,nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearlity={'RNN_TANH':'tanh','RELU':'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError(''' an valid option for \"--model\" was supplied , options are \n",
    "                                 ['LSTM','GRU','RNN_TANH','RNN_RELU']. ''')\n",
    "            self.rnn=nn.RNN(ninp,nhid,nlayers,nonlinearity=nonlinearity,dropout=dropout)\n",
    "        self.decode=nn.Linear(nhid,ntoken)\n",
    "        self.init_weights()\n",
    "        #将后面会需要用到的几个层加入到属性上来\n",
    "        self.rnn_type=rnn_type\n",
    "        self.nhid=nhid\n",
    "        self.nlayers=nlayers\n",
    "    def init_weights(self):\n",
    "        #需要初始化的层是encoder和decoder。\n",
    "        intrange=0.1\n",
    "        self.encode.weight.data.uniform_(-intrange,intrange)\n",
    "        self.decode.bias.data.zero_()\n",
    "        self.decode.weight.data.uniform_(-intrange,intrange)\n",
    "    def forward(self,input,hidden):\n",
    "        emb=self.dropout(self.encode(input))\n",
    "        output,hidden=self.rnn(emb,hidden)#注意不是直接像上面那样在这里直接就dropout，而只是对输出dropout。\n",
    "        output=self.dropout(output)\n",
    "        \n",
    "        decoded=self.decode(output.view(output.size(0)*output.size(1),output.size(2)))\n",
    "        return decoded.view(output.size(0),output.size(1),decoded.size(1)),hidden\n",
    "    #初始化权重层，若是LSTM则比较特殊一点\n",
    "    def init_hidden(self,bsz,requires_grad=True):\n",
    "        weight=next(self.parameters())\n",
    "        if self.rnn_type =='LSTM':\n",
    "            return (weight.new_zeros((self.nlayers,bsz,self.nhid),requires_grad=requires_grad),\n",
    "                   weight.new_zeros((self.nlayers,bsz,self.nhid),requires_grad=requires_grad))\n",
    "        else:\n",
    "            return weight.new_zeros((self.nlayers,bsz,self.nhid),requires_grad=requires_grad)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RNNModel('LSTM',VOCAB_SIZE,EMBEDDING_SIZE,EMBEDDING_SIZE,2,dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "下面定义的这个函数可了不得了，，可以将一个tensor从一张计算图中分离出来，比如将隐层单独拿出来后，仅仅用于预测！！！\n",
    "'''\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h,torch.torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型评估代码，这个不是很熟悉，经常写模型训练代码，评估代码没注意过。\n",
    "#这个地方比较特殊，在mode计算的时候，有一个hidden需要传入，但是之前的hidden\n",
    "#是计算了梯度的，所以这里需要重新初始化一个hidden，不需要传入梯度，新生成的需要从原始图中摘除。\n",
    "def evaluate(model,data):\n",
    "    \n",
    "    model.eval()\n",
    "    total_count=0.\n",
    "    total_loss=0.\n",
    "    it=iter(data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden=model.init_hidden(BATCH_SIZE,requires_grad=False)\n",
    "        for i,batch in enumerate(it):\n",
    "            data,target=batch.text,batch.target\n",
    "            if USE_CUDA:\n",
    "                data,target=data.cuda(),target.cuda()\n",
    "            hidden=repackage_hidden(hidden)\n",
    "            with torch.no_grad():\n",
    "                output,hidden=model(data,hidden)\n",
    "            loss=loss_fn(output.view(-1,VOCAB_SIZE),target.view(-1))\n",
    "            total_count+=np.multiply(*data.size())\n",
    "            total_loss+=loss.item()*np.multiply(*data.size())\n",
    "    loss=total_loss/total_count\n",
    "    model.train()\n",
    "    return loss\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "return arrays must be of ArrayType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9433c7c52e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: return arrays must be of ArrayType"
     ]
    }
   ],
   "source": [
    "data=torch.Tensor(1,2,3).cuda()\n",
    "print(data.size())\n",
    "print(np.multiply(*data.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "lr=0.001\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer,0.5)#损失不下降的时候，就设置这个东西，让学习率下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 5.883502006530762\n",
      "best model,val loss:  5.257648636355273\n",
      "epoch 0 iter 1000 loss 5.592380523681641\n",
      "epoch 0 iter 2000 loss 5.630694389343262\n",
      "epoch 0 iter 3000 loss 5.557417392730713\n",
      "epoch 0 iter 4000 loss 4.943767547607422\n",
      "epoch 0 iter 5000 loss 5.499133110046387\n",
      "epoch 0 iter 6000 loss 5.375182151794434\n",
      "epoch 0 iter 7000 loss 5.286401271820068\n",
      "epoch 0 iter 8000 loss 5.407065391540527\n",
      "epoch 0 iter 9000 loss 5.166967391967773\n",
      "epoch 0 iter 10000 loss 5.30166482925415\n",
      "best model,val loss:  5.017548044417916\n",
      "epoch 0 iter 11000 loss 5.508561611175537\n",
      "epoch 0 iter 12000 loss 5.507205963134766\n",
      "epoch 0 iter 13000 loss 5.266716003417969\n",
      "epoch 0 iter 14000 loss 5.1745285987854\n",
      "epoch 1 iter 0 loss 5.521990776062012\n",
      "best model,val loss:  4.958719471625291\n",
      "epoch 1 iter 1000 loss 5.35995626449585\n",
      "epoch 1 iter 2000 loss 5.473919868469238\n",
      "epoch 1 iter 3000 loss 5.343467712402344\n",
      "epoch 1 iter 4000 loss 4.811668872833252\n",
      "epoch 1 iter 5000 loss 5.355369567871094\n",
      "epoch 1 iter 6000 loss 5.311069488525391\n",
      "epoch 1 iter 7000 loss 5.203505039215088\n",
      "epoch 1 iter 8000 loss 5.276126861572266\n",
      "epoch 1 iter 9000 loss 5.015072822570801\n",
      "epoch 1 iter 10000 loss 5.155297756195068\n",
      "best model,val loss:  4.869142244020979\n",
      "epoch 1 iter 11000 loss 5.342475891113281\n",
      "epoch 1 iter 12000 loss 5.338865280151367\n",
      "epoch 1 iter 13000 loss 5.1017937660217285\n",
      "epoch 1 iter 14000 loss 5.053702354431152\n"
     ]
    }
   ],
   "source": [
    "#模型训练代码\n",
    "GRAD_CLIP=1\n",
    "NUM_EPOCHS=2\n",
    "val_losses=[]\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    it=iter(train_iter)\n",
    "    hidden=model.init_hidden(BATCH_SIZE)\n",
    "    for i,batch in enumerate(it):\n",
    "        data,target=batch.text,batch.target\n",
    "        if USE_CUDA:\n",
    "            data,target=data.cuda(),target.cuda()          \n",
    "        hidden=repackage_hidden(hidden)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output,hidden=model(data,hidden)\n",
    "        \n",
    "        loss=loss_fn(output.view(-1,VOCAB_SIZE),target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        if i%1000==0:\n",
    "            print('epoch',epoch,'iter',i,'loss',loss.item())\n",
    "            \n",
    "        if i%10000==0:\n",
    "            val_loss=evaluate(model,val_iter)\n",
    "            if len(val_losses)==0 or val_loss<min(val_losses):\n",
    "                print('best model,val loss: ',val_loss)\n",
    "                torch.save(model.state_dict(),'/home/control/Desktop/text8/lm-best.pth')\n",
    "            else:\n",
    "                sheduler.step()\n",
    "                optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "            val_losses.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model=RNNModel('LSTM',VOCAB_SIZE,EMBEDDING_SIZE,EMBEDDING_SIZE,2,dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    best_model=best_model.cuda()\n",
    "best_model.load_state_dict(torch.load('/home/control/Desktop/text8/lm-best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preplexity:  130.20918127859724\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(best_model,val_iter)\n",
    "print('preplexity: ',np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preplexity:  165.65304238321227\n"
     ]
    }
   ],
   "source": [
    "test_loss=evaluate(best_model,test_iter)\n",
    "print('preplexity: ',np.exp(test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 fizz\n",
      "4 4\n",
      "5 buzz\n",
      "15 fizzbuzz\n"
     ]
    }
   ],
   "source": [
    "#写一段代码实现一个fizz-buzz-fizzbuzz的小游戏\n",
    "#从1开始往上数，3的倍数输出fizz，5的倍数输出buzz，15的倍数输出fizzbuzz，\n",
    "#编码:就是将对应的输出规律变成计算机可识别的数字\n",
    "def encode(i):\n",
    "    if i%15==0:return 3\n",
    "    elif i%5==0:return 2\n",
    "    elif i%3==0:return 1\n",
    "    else: return 0\n",
    "#解码：就是将对应输出的编码找到针对真正对应的输出进行输出\n",
    "def decode(i,prediction):\n",
    "    return [str(i),'fizz','buzz','fizzbuzz'][prediction]\n",
    "print(1,decode(1,encode(1)))\n",
    "print(2,decode(2,encode(2)))\n",
    "print(3,decode(3,encode(3)))\n",
    "print(4,decode(4,encode(4)))\n",
    "print(5,decode(5,encode(5)))\n",
    "print(15,decode(15,encode(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#搭建一个神经网络实现上述游戏\n",
    "import torch\n",
    "import numpy as np\n",
    "NUM_DIGITS=10#将输入表示成10位的二进制格式，\n",
    "def binary_encode(i,num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "print(binary_encode(1000,10))#1111111000\n",
    "#这里是有问题的吧，将数字127转化成二进制后，应该是0001111111，所以导致后面的训练是不对的，所以这个函数有问题！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=torch.Tensor([binary_encode(i,NUM_DIGITS) for i in range(101,2**NUM_DIGITS)])\n",
    "trainY=torch.LongTensor([encode(i) for i in range(101,2**NUM_DIGITS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([923, 10])\n",
      "torch.Size([923])\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "#print(len(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建神经网络\n",
    "NUM_H=100\n",
    "model=torch.nn.Sequential(\n",
    "          torch.nn.Linear(NUM_DIGITS,NUM_H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(NUM_H,4))#最后就是4分类的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 1.1678415536880493\n",
      "epoch 1000 loss 0.5117358565330505\n",
      "epoch 2000 loss 0.1281355619430542\n",
      "epoch 3000 loss 0.04216296598315239\n",
      "epoch 4000 loss 0.01959412358701229\n",
      "epoch 5000 loss 0.011504154652357101\n",
      "epoch 6000 loss 0.00762152997776866\n",
      "epoch 7000 loss 0.005540934391319752\n",
      "epoch 8000 loss 0.004279635846614838\n",
      "epoch 9000 loss 0.003483657957985997\n"
     ]
    }
   ],
   "source": [
    "#训练网络\n",
    "BATCH_SIZE=128\n",
    "for epoch in range(10000):\n",
    "    for start in range(0,len(trainY),BATCH_SIZE):\n",
    "#         x=trainX[start:start+BATCH_SIZE]\n",
    "#         y_pred=model(x)\n",
    "#         loss=loss_fn(y_pred,train_Y[start:start+BATCH_SIZE])\n",
    "        end=start+BATCH_SIZE\n",
    "        batchX=trainX[start:end]\n",
    "        batchY=trainY[start:end]\n",
    "        y_pred=model(batchX)\n",
    "        loss=loss_fn(y_pred,batchY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%1000==0:\n",
    "        print('epoch',epoch,'loss',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用训练好的模型，对1-100的数字进行预测。\n",
    "testX=torch.Tensor([binary_encode(i,NUM_DIGITS) for i in range(1,101)])\n",
    "with torch.no_grad():\n",
    "    y_pred=model(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', 'fizz', '22', '23', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', '68', '69', 'buzz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', '81', '82', '83', '84', 'buzz', '86', '87', '88', '89', 'fizzbuzz', '91', '92', '93', '94', 'buzz', 'fizz', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "#print(type(y_pred))\n",
    "#print(y_pred.max(1))\n",
    "#print(y_pred.max(1)[1].data.tolist())\n",
    "predictions = zip(range(1,101),y_pred.max(1)[1].data.tolist())\n",
    "print([decode(i,x) for (i,x) in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_pred.max(1)[1].numpy()==np.array([encode(i) for i in range(1,101)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the second time \n",
    "def encode(i):\n",
    "    if i%15==0: return 3\n",
    "    elif i%5==0: return 2\n",
    "    elif i%3==0: return 1\n",
    "    else: return 0\n",
    "def decode(i,prediction):\n",
    "    return [str(i),'fizz','buzz','fizzbuzz'][prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 fizz\n",
      "5 buzz\n",
      "15 fizzbuzz\n"
     ]
    }
   ],
   "source": [
    "print(1,decode(1,encode(1)))\n",
    "print(3,decode(3,encode(3)))\n",
    "print(5,decode(5,encode(5)))\n",
    "print(15,decode(15,encode(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([923, 10])\n",
      "torch.Size([923])\n"
     ]
    }
   ],
   "source": [
    "#将十进制的数字编码成num_digits位的数字。\n",
    "def binary_encode_(x,num_digits):\n",
    "    return np.array([x>>d & 1 for d in range(num_digits)])\n",
    "NUM_DIGITS=10\n",
    "trainx=torch.Tensor([binary_encode(x,NUM_DIGITS) for x in range(101,2**NUM_DIGITS)])\n",
    "trainy=torch.LongTensor([encode(i) for i in range(101,2**NUM_DIGITS)])\n",
    "print(trainx.shape)\n",
    "print(trainy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIGITS=10\n",
    "NUM_H=100\n",
    "model=torch.nn.Sequential(\n",
    "       torch.nn.Linear(NUM_DIGITS,NUM_H),\n",
    "       torch.nn.ReLU(),\n",
    "       torch.nn.Linear(NUM_H,4))\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 1.1490232944488525\n",
      "epoch 1000 loss 0.38249239325523376\n",
      "epoch 2000 loss 0.06652012467384338\n",
      "epoch 3000 loss 0.028530113399028778\n",
      "epoch 4000 loss 0.01735602878034115\n",
      "epoch 5000 loss 0.010908828116953373\n",
      "epoch 6000 loss 0.007348937448114157\n",
      "epoch 7000 loss 0.0051686023361980915\n",
      "epoch 8000 loss 0.0039175162091851234\n",
      "epoch 9000 loss 0.0031071139965206385\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "for epoch in range(10000):\n",
    "    for start in range(0,len(trainy),batch_size):\n",
    "        end=start+batch_size\n",
    "        batchx=trainx[start:end]\n",
    "        batchy=trainy[start:end]\n",
    "        y_pred=model(batchx)\n",
    "        loss=loss_fn(y_pred,batchy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%1000==0:\n",
    "        print('epoch',epoch,'loss',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', '9', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', '21', '22', '23', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', '42', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', '68', 'fizz', 'buzz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', '81', '82', '83', 'fizz', 'buzz', '86', '87', '88', '89', 'fizzbuzz', '91', '92', '93', '94', 'buzz', 'fizz', '97', '98', 'fizz', 'buzz']\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "testx=torch.Tensor([binary_encode(i,NUM_DIGITS) for i in range(1,101)])\n",
    "with torch.no_grad():\n",
    "    y_pred=model(testx)\n",
    "predictions=zip(range(1,101),y_pred.max(1)[1].data.tolist())\n",
    "print([decode(i,x) for (i,x) in predictions])\n",
    "print(np.sum(y_pred.max(1)[1].numpy()==np.array([encode(i) for i in range(1,101)])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

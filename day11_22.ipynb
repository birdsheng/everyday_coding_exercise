{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习手动搭建两层全连接神经网络\n",
    "基本思路：\n",
    "1.上来无从下手两眼发蒙，正常，典型的写程序少，没动过脑子，do not be scared and just trust yourself!\n",
    "2.需要设置输入层、隐层、输出层的神经元的个数，设置输入，输出，权重矩阵。\n",
    "3.进行n轮的epoch的迭代，关键是在每一次迭代中实现对前向和反向的计算过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/control/everyday_coding_exercise/day11_22/everyday_coding_exercise\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30530526.097928338\n"
     ]
    }
   ],
   "source": [
    "#第一种方法：完全用numpy实现。\n",
    "import numpy as np\n",
    "N,D_IN,D_H,D_OUT=64,1000,100,10\n",
    "x=np.random.randn(N,D_IN)\n",
    "y=np.random.randn(N,D_OUT)\n",
    "#w1=np.random.rand（D_IN,D_H）\n",
    "w1=np.random.randn(D_IN,D_H)\n",
    "w2=np.random.randn(D_H,D_OUT)\n",
    "lr=1e-6\n",
    "for i in range(1):\n",
    "    h=x.dot(w1)\n",
    "    h_relu=np.maximum(0,h)#maximum()输入至少两个，比较对应位置，返回较大的值，np.max()输入是一个，返回最大的数\n",
    "    y_pred=h_relu.dot(w2)\n",
    "    \n",
    "    loss=np.square(y_pred-y).sum()\n",
    "    print(i,loss)\n",
    "    #反向传播\n",
    "    grad_y_pred=2*(y_pred-y)#从loss往第一层进行求导,该维度为（64,10），数loss对输出10维进行求导\n",
    "    #print(np.shape(grad_y_pred))\n",
    "    #grad_w2=grad_y_pred.dot(h_relu.T)#在grad_y_pred基础上进行下一层求导，这里涉及矩阵求导公式\n",
    "    #上面的写法，不符合矩阵的规矩，虽然求导公式是这样的，注意维度！\n",
    "    grad_w2=h_relu.T.dot(grad_y_pred)\n",
    "    #print(np.shape(grad_w2))#(100,10)\n",
    "    grad_h_relu=grad_y_pred.dot(w2.T)\n",
    "    #print(np.shape(grad_h_relu))#(64,100)\n",
    "    grad_h=grad_h_relu.copy()#深拷贝，对对象及其子对象都进行copy一份，对新生成的对象修改删除操作不会影响到原对象。\n",
    "    #print(grad_h<0)得到一个true、false索引矩阵\n",
    "    grad_h[grad_h<0]=0\n",
    "    #grad_w1=grad_h.dot(x.T)#这里数学上公式是这样，但是不符合计算维度，错误同上\n",
    "    grad_w1=x.T.dot(grad_h)\n",
    "    #print(np.shape(grad_w1))#shape为(1000,100)\n",
    "    w1-=lr*grad_w1#得到的grad_w1的维度正好和w1本身的维度一样。\n",
    "    w2-=lr*grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29053692.0\n"
     ]
    }
   ],
   "source": [
    "#第二种方法：使用pytorch实现上面的一段代码\n",
    "import torch\n",
    "device=torch.device('cuda')\n",
    "type=torch.float\n",
    "N,D_IN,D_H,D_OUT=64,1000,100,10\n",
    "x=torch.randn(N,D_IN,dtype=type,device=device)\n",
    "y=torch.randn(N,D_OUT,dtype=type,device=device)\n",
    "w1=torch.randn(D_IN,D_H,dtype=type,device=device)\n",
    "w2=torch.randn(D_H,D_OUT,dtype=type,device=device)\n",
    "lr=1e-6\n",
    "for i in range(1):\n",
    "    h=x.mm(w1)#.mm就是矩阵相乘\n",
    "    h_relu=torch.clamp(h,min=0)#在pytorch官网的doc手册上进行查找，可知道每个api的含义，clamp有两种用法，按最大最小截取和按最小截取。\n",
    "    y_pred=h_relu.mm(w2)\n",
    "    \n",
    "    loss=(y_pred-y).pow(2).sum().item()#官网手册给出的函数是torch.pow(),但是这里使用的是.pow()\n",
    "    print(i,loss)\n",
    "    grad_y_pred=2*(y_pred-y)\n",
    "    grad_w2=h_relu.t().mm(grad_y_pred)#torch.t()表示对小于等于2维的矩阵进行转置\n",
    "    grad_h_relu=grad_y_pred.mm(w2.t())\n",
    "    grad_h=grad_h_relu.clone()\n",
    "    grad_h[grad_h<0]=0\n",
    "    grad_w1=x.t().mm(grad_h)\n",
    "    \n",
    "    w1-=lr*grad_w1\n",
    "    w2-=lr*grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36386432.0\n"
     ]
    }
   ],
   "source": [
    "#第三种方法：对方法2进行改进，因为pytorch最大的优势是可以autograd，一个tensor是计算图中的一个节点，若x是一个tensor\n",
    "#且x.requires_grad=True,则反向传播以后，x.grad包含着loss对于该tensor的grad信息。\n",
    "import torch\n",
    "device=torch.device('cuda')\n",
    "type=torch.float\n",
    "N,D_IN,D_H,D_OUT=64,1000,100,10\n",
    "x=torch.randn(N,D_IN,dtype=type,device=device)\n",
    "y=torch.randn(N,D_OUT,dtype=type,device=device)\n",
    "w1=torch.randn(D_IN,D_H,dtype=type,device=device,requires_grad=True)\n",
    "w2=torch.randn(D_H,D_OUT,dtype=type,device=device,requires_grad=True)\n",
    "lr=1e-6\n",
    "for i in range(1):\n",
    "    #h=x.mm(w1)#.mm就是矩阵相乘\n",
    "    #h_relu=torch.clamp(h,min=0)#在pytorch官网的doc手册上进行查找，可知道每个api的含义，clamp有两种用法，按最大最小截取和按最小截取。\n",
    "    #y_pred=h_relu.mm(w2)\n",
    "    \n",
    "    y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss=(y_pred-y).pow(2).sum()#官网手册给出的函数是torch.pow(),但是这里使用的是.pow()\n",
    "    print(i,loss.item())\n",
    "#     grad_y_pred=2*(y_pred-y)\n",
    "#     grad_w2=h_relu.t().mm(grad_y_pred)#torch.t()表示对小于等于2维的矩阵进行转置\n",
    "#     grad_h_relu=grad_y_pred.mm(w2.t())\n",
    "#     grad_h=grad_h_relu.clone()\n",
    "#     grad_h[grad_h<0]=0\n",
    "#     grad_w1=x.t().mm(grad_h)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():#在这个模式下，所有的每一步都不计算grad，即使输入的requires_grad=True\n",
    "        w1-=lr*w1.grad\n",
    "        w2-=lr*w2.grad\n",
    "        \n",
    "        #因为在下一次迭代的时候，不需要知道w1的grad，也不需要在w1的grad上进行叠加，所以需要将其grad置0.\n",
    "        w1.grad.zero_()#没有查到这个api啊？？？\n",
    "        w2.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 644.18701171875\n"
     ]
    }
   ],
   "source": [
    "#第四种方法：在构造网络过程中，调用torch.nn进行构造\n",
    "import torch\n",
    "# device=torch.device('cuda')\n",
    "# type=torch.float\n",
    "N,D_IN,D_H,D_OUT=64,1000,100,10\n",
    "x=torch.randn(N,D_IN)\n",
    "y=torch.randn(N,D_OUT)\n",
    "# w1=torch.randn(D_IN,D_H,dtype=type,device=device,requires_grad=True)\n",
    "# w2=torch.randn(D_H,D_OUT,dtype=type,device=device,requires_grad=True)\n",
    "model=torch.nn.Sequential(\n",
    "      torch.nn.Linear(D_IN,D_H),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(D_H,D_OUT)\n",
    ")\n",
    "loss_fn=torch.nn.MSELoss(reduction='sum')\n",
    "lr=1e-4\n",
    "for i in range(1):\n",
    "    #h=x.mm(w1)#.mm就是矩阵相乘\n",
    "    #h_relu=torch.clamp(h,min=0)#在pytorch官网的doc手册上进行查找，可知道每个api的含义，clamp有两种用法，按最大最小截取和按最小截取。\n",
    "    #y_pred=h_relu.mm(w2)\n",
    "    \n",
    "    #y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    y_pred=model(x)\n",
    "    \n",
    "    #loss=(y_pred-y).pow(2).sum()#官网手册给出的函数是torch.pow(),但是这里使用的是.pow()\n",
    "    loss=loss_fn(y_pred,y)\n",
    "    print(i,loss.item())\n",
    "#     grad_y_pred=2*(y_pred-y)\n",
    "#     grad_w2=h_relu.t().mm(grad_y_pred)#torch.t()表示对小于等于2维的矩阵进行转置\n",
    "#     grad_h_relu=grad_y_pred.mm(w2.t())\n",
    "#     grad_h=grad_h_relu.clone()\n",
    "#     grad_h[grad_h<0]=0\n",
    "#     grad_w1=x.t().mm(grad_h)\n",
    "    #进行反向传播之前，先对模型进行梯度置0，为了将这一次的梯度，不累加到下一次上。\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():#在这个模式下，所有的每一步都不计算grad，即使输入的requires_grad=True\n",
    "#         w1-=lr*w1.grad\n",
    "#         w2-=lr*w2.grad\n",
    "#因为模型已经是用nn.Sequential()搭建的了，所以就不能用上述方式进行模型更新了。\n",
    "        for params in model.parameters():\n",
    "            #print(params)#通过打印可知道，这里的parameters就是w1.weight,b1,w2.weight,b2\n",
    "            params-=lr*params.grad\n",
    "        \n",
    "        #因为在下一次迭代的时候，不需要知道w1的grad，也不需要在w1的grad上进行叠加，所以需要将其grad置0.\n",
    "#         w1.grad.zero_()#没有查到这个api啊？？？\n",
    "#         w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 614.8849487304688\n"
     ]
    }
   ],
   "source": [
    "#第五种方法：在构造网络过程中，调用torch.nn进行构造，更新参数的时候，不手动更新，而是自动更新参数(optimizer)\n",
    "import torch\n",
    "# device=torch.device('cuda')\n",
    "# type=torch.float\n",
    "N,D_IN,D_H,D_OUT=64,1000,100,10\n",
    "x=torch.randn(N,D_IN)\n",
    "y=torch.randn(N,D_OUT)\n",
    "# w1=torch.randn(D_IN,D_H,dtype=type,device=device,requires_grad=True)\n",
    "# w2=torch.randn(D_H,D_OUT,dtype=type,device=device,requires_grad=True)\n",
    "model=torch.nn.Sequential(\n",
    "      torch.nn.Linear(D_IN,D_H),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(D_H,D_OUT)\n",
    ")\n",
    "loss_fn=torch.nn.MSELoss(reduction='sum')\n",
    "lr=1e-4\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr)\n",
    "for i in range(1):\n",
    "    #h=x.mm(w1)#.mm就是矩阵相乘\n",
    "    #h_relu=torch.clamp(h,min=0)#在pytorch官网的doc手册上进行查找，可知道每个api的含义，clamp有两种用法，按最大最小截取和按最小截取。\n",
    "    #y_pred=h_relu.mm(w2)\n",
    "    \n",
    "    #y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    y_pred=model(x)\n",
    "    \n",
    "    #loss=(y_pred-y).pow(2).sum()#官网手册给出的函数是torch.pow(),但是这里使用的是.pow()\n",
    "    loss=loss_fn(y_pred,y)\n",
    "    print(i,loss.item())\n",
    "#     grad_y_pred=2*(y_pred-y)\n",
    "#     grad_w2=h_relu.t().mm(grad_y_pred)#torch.t()表示对小于等于2维的矩阵进行转置\n",
    "#     grad_h_relu=grad_y_pred.mm(w2.t())\n",
    "#     grad_h=grad_h_relu.clone()\n",
    "#     grad_h[grad_h<0]=0\n",
    "#     grad_w1=x.t().mm(grad_h)\n",
    "    #进行反向传播之前，先对模型进行梯度置0，为了将这一次的梯度，不累加到下一次上。\n",
    "    model.zero_grad() #貌似这里调用哪一个方法进行梯度清零都可以...\n",
    "    #optimizer.zero_grad()#切记！！在反向传播之前一定将梯度置零，因为在backward的时候，grad默认是累加的！！，在每一次迭代过程中并不需要\n",
    "    loss.backward()\n",
    "    optimizer.step()#直接调用这个方法，就是逐步更新梯度了\n",
    "#     with torch.no_grad():#在这个模式下，所有的每一步都不计算grad，即使输入的requires_grad=True\n",
    "# #         w1-=lr*w1.grad\n",
    "# #         w2-=lr*w2.grad\n",
    "# #因为模型已经是用nn.Sequential()搭建的了，所以就不能用上述方式进行模型更新了。\n",
    "#         for params in model.parameters():\n",
    "#             #print(params)#通过打印可知道，这里的parameters就是w1.weight,b1,w2.weight,b2\n",
    "#             params-=lr*params.grad\n",
    "        \n",
    "        #因为在下一次迭代的时候，不需要知道w1的grad，也不需要在w1的grad上进行叠加，所以需要将其grad置0.\n",
    "#         w1.grad.zero_()#没有查到这个api啊？？？\n",
    "#         w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 699.8203125\n"
     ]
    }
   ],
   "source": [
    "#第六种方法：定义一个模型（类对象），继承自nn.Module,当需要定义一个比nn.Sequential()更加复杂的模型的时候，需要这么做！\n",
    "import torch\n",
    "# device=torch.device('cuda')\n",
    "# type=torch.float\n",
    "N,D_IN,D_H,D_OUT=64,1000,100,10\n",
    "x=torch.randn(N,D_IN)\n",
    "y=torch.randn(N,D_OUT)\n",
    "class TwoLayrNet(torch.nn.Module):\n",
    "    def __init__(self,D_IN,D_H,D_OUT):#仅需要考虑一个样本输入下的情形，也就是不需要传入参数N\n",
    "        super(TwoLayrNet,self).__init__()\n",
    "        self.linear1=torch.nn.Linear(D_IN,D_H)\n",
    "        self.linear2=torch.nn.Linear(D_H,D_OUT)\n",
    "    def forward(self,x):\n",
    "        h_relu=self.linear1(x).clamp(min=0)\n",
    "        y_pred=self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "model=TwoLayrNet(D_IN,D_H,D_OUT)\n",
    "loss_fn=torch.nn.MSELoss(reduction='sum')\n",
    "lr=1e-4\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr)\n",
    "\n",
    "for i in range(1):\n",
    "    y_pred=model(x) \n",
    "    loss=loss_fn(y_pred,y)\n",
    "    print(i,loss.item())\n",
    "    optimizer.zero_grad()#切记！！在反向传播之前一定将梯度置零，因为在backward的时候，grad默认是累加的！！，在每一次迭代过程中并不需要\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
